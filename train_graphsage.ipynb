{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf42225a-c5bc-49a9-a799-a038f2d56bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_DIR = \"/nfs/rohit/ALL_LOGS\"\n",
    "OUTPUT_DIR = \"/nfs/rohit/models2/\"\n",
    "RUN_NUMBER = 2\n",
    "\n",
    "RESULT_DIR = os.path.join(OUTPUT_DIR, f\"run_{RUN_NUMBER}\")\n",
    "\n",
    "os.mkdir(RESULT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5fb70d-5c3a-40a7-81ab-94aeebc99c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, \"pyg_simplified.pkl\"), 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33c1e9-4939-40af-adff-0619079ba111",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_e2e_times = pd.read_csv(os.path.join(DATA_DIR, \"all_e2e_times.csv\"), names=['file', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed686c-097c-4098-9d1f-42b340b66de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datapoint(x):\n",
    "    assert x.endswith(\".log\")\n",
    "    filename = x[:-4]\n",
    "    info = filename.split('_')\n",
    "    assert len(info) == 6, f\"{x}\"\n",
    "    data[x]['program'] = info[0]\n",
    "    data[x]['cpu'] = info[1].replace('cpu', '')\n",
    "    data[x]['mem'] = info[2].replace('mem', '')\n",
    "    data[x]['file'] = info[3]\n",
    "    data[x]['lvl'] = info[4].replace('lvl', '')\n",
    "    data[x]['run'] = info[5].replace('run', '')\n",
    "    data[x]['e2e_time'] = all_e2e_times[all_e2e_times['file'] == f\"{filename}.info\"]['time'].item()\n",
    "    return data[x]\n",
    "\n",
    "dataset = list(map(process_datapoint, data.keys()))\n",
    "dataset = list(filter(lambda x: x['cpu'] in ['0.1', '0.3', '0.5', '0.7', '1.0'], dataset))\n",
    "\n",
    "X_train, X_test = train_test_split(dataset, test_size = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d0cee-1b19-4a7b-851a-0a7ee050e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = torch.concat(tuple(map(lambda y: y.x, X_train)))\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(all_features)\n",
    "\n",
    "# assert not os.path.exists(os.path.join(RESULT_DIR, 'scaler.pkl'))\n",
    "with open(os.path.join(RESULT_DIR, 'scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Group all training graphs into a single graph to perform sampling\n",
    "train_graphs = torch_geometric.data.Batch.from_data_list(list(X_train))\n",
    "train_graphs.x = scaler.transform(train_graphs.x).astype(np.float32)\n",
    "\n",
    "test_graphs = torch_geometric.data.Batch.from_data_list(list(X_test))\n",
    "test_graphs.x = scaler.transform(test_graphs.x).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a9485-9713-4e4e-ad22-b506414c7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'BATCH_SIZE': 65536,\n",
    "    'NUM_NEIGHBORS': [10, 10],\n",
    "    'HIDDEN_CHANNELS': 64,\n",
    "    'NUM_LAYERS': 5,\n",
    "    'OUT_CHANNELS': 32,\n",
    "    'LR': 0.01,\n",
    "    'NUM_EPOCHS': 25\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULT_DIR, 'hyperparams.json'), 'w') as f:\n",
    "    json.dump(hyperparams, f)\n",
    "\n",
    "train_loader = torch_geometric.loader.LinkNeighborLoader(train_graphs, batch_size=hyperparams['BATCH_SIZE'], shuffle=True,\n",
    "                            neg_sampling_ratio=1.0, num_neighbors=hyperparams['NUM_NEIGHBORS'],\n",
    "                            num_workers=6, persistent_workers=True)\n",
    "\n",
    "eval_loader  = torch_geometric.loader.LinkNeighborLoader(test_graphs, batch_size=hyperparams['BATCH_SIZE'], shuffle=True,\n",
    "                            neg_sampling_ratio=1.0, num_neighbors=hyperparams['NUM_NEIGHBORS'],\n",
    "                            num_workers=6, persistent_workers=True)\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i].x = torch.from_numpy(scaler.transform(X_train[i].x).astype(np.float32))\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i].x = torch.from_numpy(scaler.transform(X_test[i].x).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8bb2b-0ca1-4c96-873c-dedad0fb92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "model = torch_geometric.nn.GraphSAGE(\n",
    "    in_channels=train_graphs.num_features,\n",
    "    hidden_channels=hyperparams['HIDDEN_CHANNELS'],\n",
    "    num_layers=hyperparams['NUM_LAYERS'],\n",
    "    out_channels=hyperparams['OUT_CHANNELS'],\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c493bad-3345-4f7b-8e17-4b25b617179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphsage_unsup import train, evaluate\n",
    "\n",
    "for epoch in range(1, hyperparams['NUM_EPOCHS']):\n",
    "    start_train = time.time()\n",
    "    loss = train(model, optimizer, train_loader, device)\n",
    "    end_train = time.time()\n",
    "\n",
    "    start_val = time.time()\n",
    "    val_loss = evaluate(model, optimizer, eval_loader, device)\n",
    "    end_val = time.time()\n",
    "    \n",
    "    logline = {\n",
    "        'epoch': epoch,\n",
    "        'loss': loss,\n",
    "        'val_loss': val_loss,\n",
    "        'train_time': end_train - start_train,\n",
    "        'val_time': end_val - start_val\n",
    "    }\n",
    "    with open(os.path.join(RESULT_DIR, 'logs.json'), 'a') as f:\n",
    "        json.dump(logline, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    print(logline)\n",
    "    torch.save(model.state_dict(), os.path.join(RESULT_DIR, f\"ckpt_{epoch}.pt\"))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
