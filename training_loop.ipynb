{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu126\n",
      "PyTorch Geometric version: 2.7.0\n",
      "PyTorch Lightning version: 2.5.0.post0\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from typing import Dict, List, Tuple\n",
    "from torch.nn import BatchNorm1d\n",
    "from torchmetrics import Accuracy\n",
    "from torch_geometric.data.lightning import LightningNodeData\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch import Tensor\n",
    "import torch_geometric.transforms as T\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "from torch_geometric.nn import Linear, SAGEConv, to_hetero\n",
    "from torch_geometric.typing import EdgeType, NodeType\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels: int, out_channels: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.lin = Linear(-1, out_channels)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "class Model(LightningModule):\n",
    "    def __init__(self,\n",
    "        metadata: Tuple[List[NodeType], List[EdgeType]],\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        dropout: float):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        model = GNN(hidden_channels, out_channels, dropout)\n",
    "\n",
    "        # Convert the homogeneous GNN model to a heterogeneous variant in\n",
    "        # which distinct parameters are learned for each node and edge type.\n",
    "        self.model = to_hetero(model, metadata, aggr='sum')\n",
    "\n",
    "        self.train_acc = Accuracy(task='multiclass', num_classes=out_channels)\n",
    "        self.val_acc = Accuracy(task='multiclass', num_classes=out_channels)\n",
    "        self.test_acc = Accuracy(task='multiclass', num_classes=out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_dict: Dict[NodeType, Tensor],\n",
    "        edge_index_dict: Dict[EdgeType, Tensor],\n",
    "    ) -> Dict[NodeType, Tensor]:\n",
    "        return self.model(x_dict, edge_index_dict)\n",
    "\n",
    "    def common_step(self, batch: Batch) -> Tuple[Tensor, Tensor]:\n",
    "        node_type = 'paper'\n",
    "        batch_size = batch[node_type].batch_size\n",
    "        y_hat = self(batch.x_dict, batch.edge_index_dict)[node_type][:batch_size]\n",
    "        y = batch[node_type].y[:batch_size]\n",
    "        return y_hat, y\n",
    "\n",
    "    def training_step(self, batch: Batch, batch_idx: int) -> Tensor:\n",
    "        node_type = 'paper'\n",
    "        y_hat, y = self.common_step(batch)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.train_acc(y_hat.softmax(dim=-1), y)\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True, on_step=False,\n",
    "                 on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Batch, batch_idx: int):\n",
    "        node_type = 'paper'\n",
    "        y_hat, y = self.common_step(batch)\n",
    "        self.val_acc(y_hat.softmax(dim=-1), y)\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_step=False,\n",
    "                 on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch: Batch, batch_idx: int):\n",
    "        node_type = 'paper'\n",
    "        y_hat, y = self.common_step(batch)\n",
    "        self.test_acc(y_hat.softmax(dim=-1), y)\n",
    "        self.log('test_acc', self.test_acc, prog_bar=True, on_step=False,\n",
    "                 on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "    \n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\subud\\GitHub Projects\\Graph Training\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | GraphModule        | 288 K  | train\n",
      "1 | train_acc | MulticlassAccuracy | 0      | train\n",
      "2 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "3 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "288 K     Trainable params\n",
      "0         Non-trainable params\n",
      "288 K     Total params\n",
      "1.153     Total estimated model params size (MB)\n",
      "80        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\subud\\GitHub Projects\\Graph Training\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\subud\\GitHub Projects\\Graph Training\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  18%|█▊        | 3484/19675 [07:02<32:41,  8.25it/s, v_num=5]"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('medium' or 'high')\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "dataset = OGB_MAG(osp.join('data', 'OGB'), preprocess='metapath2vec', transform=T.ToUndirected(merge=False))\n",
    "data = dataset[0]\n",
    "\n",
    "node_type = 'paper'\n",
    "\n",
    "datamodule = LightningNodeData(\n",
    "    data,\n",
    "    input_train_nodes=(node_type, data[node_type].train_mask),\n",
    "    input_val_nodes=(node_type, data[node_type].val_mask),\n",
    "    input_test_nodes=(node_type, data[node_type].test_mask),\n",
    "    loader='neighbor',\n",
    "    num_neighbors=[10, 10],\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "model = Model(data.metadata(), hidden_channels=64,\n",
    "                        out_channels=349, dropout=0.0)\n",
    "\n",
    "with torch.no_grad():  # Run a dummy forward pass to initialize lazy model\n",
    "    loader = datamodule.train_dataloader()\n",
    "    batch = next(iter(loader))\n",
    "    model.common_step(batch)\n",
    "\n",
    "strategy = pl.strategies.SingleDeviceStrategy('cuda:0')\n",
    "checkpoint = ModelCheckpoint(monitor='val_acc', save_top_k=1, mode='max')\n",
    "trainer = Trainer(strategy=strategy, devices=1, max_epochs=20,\n",
    "                    log_every_n_steps=5, callbacks=[checkpoint])\n",
    "\n",
    "trainer.fit(model, datamodule)\n",
    "trainer.test(ckpt_path='best', datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\users\\subud\\github projects\\graph training\\.venv\\lib\\site-packages (75.8.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\subud\\github projects\\graph training\\.venv\\lib\\site-packages (0.45.1)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip3 install setuptools wheel\n",
    "# Install PyTorch Geometric and related libraries\n",
    "!pip3 install torch-sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
      "Collecting torch-scatter\n",
      "  Using cached torch_scatter-2.1.2-cp312-cp312-win_amd64.whl\n",
      "Requirement already satisfied: torch-sparse in c:\\users\\subud\\github projects\\graph training\\.venv\\lib\\site-packages (0.6.18)\n",
      "Requirement already satisfied: scipy in c:\\users\\subud\\github projects\\graph training\\.venv\\lib\\site-packages (from torch-sparse) (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\subud\\github projects\\graph training\\.venv\\lib\\site-packages (from scipy->torch-sparse) (2.0.0)\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
